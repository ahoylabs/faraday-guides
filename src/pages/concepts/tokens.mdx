# Tokens and Context

Large Language Models (LLM)s generate text by calculating **which words are most likely to come next based on a given input sequence**. This calculation requires that words be converted to numbers, i.e. "tokens".

LLMâ€™s work with tokens, and compute the probability of subsequent tokens based on the given sequence of input tokens.

## Token Processing

When you send a prompt to the model, the input text is immediately tokenized. The model proceeds to predict tokens one-by-one, which are each converted back into text for us to read.

Here is a visualization of text broken into tokens:

![Tokenization](/images/tokenized_text.png)

## Context

For a given chat conversation, the subset of tokens processed by the LLM to generate the next token is called the context. The training process for a model determines the maximum context size.

LLaMa1-based models are limited to 2048 tokens of context, while LLaMa2-based models can take up to 4096 tokens of context. Because of this limit on the amount of information sent to the model (the prompt), a portion of the chat history is often removed.

On Faraday, the context is composed of model instructions, character info, chat history, and the current response.
